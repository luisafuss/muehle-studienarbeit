{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Muehle_Logic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Muehle_Utilities.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Muehle_Heuristik.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoize(f):\n",
    "    global Cache\n",
    "    \n",
    "    def f_memoized(*args):\n",
    "        key = (args[0], args[1])\n",
    "        if key in Cache:\n",
    "            return Cache[key]\n",
    "        result = f(*args)\n",
    "        Cache[key] = result\n",
    "        return result\n",
    "    \n",
    "    return f_memoized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax-Algorithmus\n",
    "\n",
    "Der Minimax-Algorithmus wird im Rahmen dieser Studienarbeit für die Ermittlung der optimalen Strategie für das Nulsummenspiel Mühle verwendet. \n",
    "Prinzip. Dabei ist der Minimax-Algorithmus ist ein relativ einfacher Algorithmus, der durch die Verwendung von Alpha-Beta-Pruning deutlich verbessert wird.\n",
    "\n",
    "Der Minimax-Algorithmus beruht auf einer Bewertungsfunktion und systematischer Suche. Gundlegend werden bei Minimax alle auf den Aktuellen Spielstatus folgenden Zustände berechnet und bewertet. Dies ist vergleichbar mit einer Baumstruktur, bei der bis zu den Blättern alle Zustände ausgewertet werden. Da dies aus Gründen der Rechenzeit und des Speichers nicht möglich ist, werden die Folgezustände nur bis zu einer gewissen Tiefe berechnet und ausgewertet. Um nur bis zu einer geringen Baumtiefe suchen zu können wird allerdings eine geeignete Heuristik benötigt. Mit der Nutzung dieser Heuristik verlieren wir allerdings die Sicherheit den optimalen Zug zu wählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `value_minimax(State, player)` erhält drei Argumente. Einen Spielzustand, einen Spieler und die Suchtiefe. Die Funktion gibt dabei den Wert zurück, den der übergebene Spielzustand für den übergeben Spieler hat.\n",
    "Dieser Wert Wert wird, für den Fall, dass das Spiel mit dem übergebenen Zustand beendet ist von def Funktion `finished()` berechnet. Wenn die maximale Suchtiefe erreicht wurde wird der Wert allerdings von der Funktion `heuristic()` berechnet.\n",
    "Ist die maximale Suchtiefe noch nicht erreicht wird rekursiv nach dem besten Folgezustand gesucht.\n",
    "Um Werte für Zustände nicht mehrfach berechnen zu müssen werden diese duch Memoisation `@memoize` zwischengespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize\n",
    "def value_minimax(state, player, depth):\n",
    "    if finished(to_list(state)):\n",
    "        return utility(to_list(state), player)\n",
    "    if depth == 0:\n",
    "        return heuristic(state, player)\n",
    "    o = opponent(player)\n",
    "    depth -= 1\n",
    "    return max([ -value_minimax(to_tuple(ns), o, depth) for ns in next_states(to_list(state), player) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `best_move_minimax(State, player)` erhält drei Argumente. Einen SPielzustand, einen Spieler und die Suchtiefe. Die Rügkabewerte sind der von der Funktion ermittelte beste Folgezustand und dessen Bewertung. Gibt es mehrere beste Folgezustände wird der Folgezustand zufällig ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move_minimax(state, player, depth):\n",
    "    ns          = next_states(state, player)\n",
    "    best_value  = value_minimax(to_tuple(state), player, depth)\n",
    "    best_moves  = [s for s in ns if -value_minimax(to_tuple(s), opponent(player), depth - 1 ) == best_value]\n",
    "    best_state  = random.choice(best_moves)\n",
    "    return best_value, best_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion minimax(State, Player) wurde erstellt, um die Funktion best_move_minimax(State, player) nach Außen eindeutiger von alpha_beta_pruning(State, Player) abzugrenzen. Die übergebenen Argumente und Rückgabewerte entsprechen somit denen der Funktion best_move_minimax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax(state, player, depth = 5):\n",
    "    return(best_move_minimax(state, player, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha-Beta-Pruning\n",
    "Das Alpha-Beta-Pruning ist, wie im Rahmen des Minimax-Algorithmus schon erwähnt eine Verbesserung von Minimax. Beim Alpha-Beta-Pruning wird die Auswertung der von Teilbäumen abgebrochen, sobald klar ist, dass keine Verbesserung erwartbar ist. Durch diese Technik wird die Rechenzeit bei steigender Suchtiefe erheblich gegenüber Minimax reduziert. \n",
    "Das Hauptkonzept von Alpha-Beta-Pruning besteht darin, die Werte Alpha und Beta über die gesamte Suche hinweg mitzunehmen. Alpha enthält dabei den bestmöglichen Wert der erkundeten Optionen für den maximierenden Spieler und Beta das gleiche für den minimierenden Spieler, wobei Alpha und Beta initial auf dem für Alpha und Beta schlechtesten Wert starten. (Alpha = -1, Beta = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Funktion `value_ab(State, player, alpha=-1, beta=1)` werden vier Argumente übergeben. Einen Spielzustand, einen Spieler, Alpha, Beta und und die Suchtiefe. Dabei gibt value_ab, wie value_minimax, den ermittelten Wert für den übergebenen Spielzustand und den übergebenen Spieler zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_ab(state, player, alpha=-1, beta=1, depth = 6):\n",
    "    global Cache\n",
    "    state = to_tuple(state)\n",
    "    if state in Cache:\n",
    "        value, a, b = Cache[state]\n",
    "        if a <= alpha and beta <= b:\n",
    "            return value\n",
    "        else:\n",
    "            alpha = min(alpha, a)\n",
    "            beta  = max(beta , b)\n",
    "            value   = alphaBeta(state, player, alpha, beta, depth=depth)\n",
    "            Cache[state] = value, alpha, beta\n",
    "            return value\n",
    "    else:\n",
    "        value = alphaBeta(state, player, alpha, beta, depth=depth)\n",
    "        Cache[state] = value, alpha, beta\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`alphaBeta(State, player, alpha, beta)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphaBeta(state, player, alpha, beta, depth):\n",
    "    state = to_list(state)\n",
    "    if finished(state):\n",
    "        return utility(state, player)\n",
    "    if depth == 0:\n",
    "        return heuristic(state, player)\n",
    "    value = alpha\n",
    "    for ns in next_states(state, player):\n",
    "        value = max(value, -value_ab(ns, opponent(player), -beta, -alpha, depth = depth-1))\n",
    "        if value >= beta:\n",
    "            return value\n",
    "        alpha = max(value, alpha)\n",
    "    return value"
   ]
  },
  {
   "source": [
    "Die Funktion `best_move_ab(state, player, depth = 6)` erhält drei Argumente. Einen SPielzustand, einen Spieler und die Suchtiefe. Die Rügkabewerte sind der von der Funktion ermittelte beste Folgezustand und dessen Bewertung. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move_ab(state, player, depth = 6):\n",
    "    ns         = next_states(state, player)\n",
    "    moves = [(-value_ab(s, opponent(player), depth), s) for s in ns]\n",
    "    (best_value, best_state) = max(moves, key=lambda x:x[0])\n",
    "    return best_value, best_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `alpha_beta_pruning(State, Player)` wurde erstellt, um die Funktion `best_move(State, player)` nach Außen eindeutiger von `minimax(State, Player)` abzugrenzen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_beta_pruning(state, player, depth = 6):\n",
    "    return(best_move_ab(state, player, depth = depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionstests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#start = time.time()\n",
    "#state = [[4, 5], [[0, 2, 0, 0, 2, 0, 0, 0], [2, 1, 1, 0, 0, 0, 0, 0], [2, 1, 1, 1, 0, 0, 0, 0]]]\n",
    "#print(alpha_beta_pruning(state, 2, 5))\n",
    "#end = time.time()\n",
    "#print(str(end-start)+'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#start = time.time()\n",
    "#state = [[4, 5], [[0, 2, 0, 0, 2, 0, 0, 0], [2, 1, 1, 0, 0, 0, 0, 0], [2, 1, 1, 1, 0, 0, 0, 0]]]\n",
    "#print(minimax(state, 2, 5))\n",
    "#end = time.time()\n",
    "#print(str(end-start)+'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#start = time.time()\n",
    "#state = [[5, 6], [[0, 0, 0, 2, 0, 0, 0, 0], [0, 1, 1, 1, 2, 0, 2, 0], [0, 1, 0, 0, 0, 0, 0, 0]]]\n",
    "#print(alpha_beta_pruning(state, 2, 5))\n",
    "#end = time.time()\n",
    "#print(str(end-start)+'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}